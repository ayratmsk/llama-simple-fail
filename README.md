**Minimum reproducible demo code llama-simple.wasm error inference**

1. Download repo
2. Create folder /models on same level with Cargo.toml
3. Download LLM-model file from HF or make symbolic link to llama-2-7b-chat.Q5_K_M.gguf
4. Run code: $ cargo run# llama-simple-fail
